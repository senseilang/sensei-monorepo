use logos::{Lexer as LogosLexer, Logos, Skip};
use neosen_data::Span;

fn lex_line_comment(lex: &mut LogosLexer<Token>) {
    let remainder = lex.remainder();
    let mut chars = remainder.char_indices().peekable();
    while chars.next_if(|&(_, c)| c != '\n').is_some() {}
    let bytes_skipped = chars.peek().map_or(remainder.len(), |&(pos, _)| pos);
    lex.bump(bytes_skipped);
}

fn lex_block_comment(lex: &mut LogosLexer<Token>) -> Result<(), ()> {
    let remainder = lex.remainder();
    let mut chars = remainder.char_indices().peekable();
    let mut depth: u32 = 1;

    while depth > 0 {
        match chars.next() {
            Some((_, '*')) if chars.next_if(|&(_, nc)| nc == '/').is_some() => depth -= 1,
            Some((_, '/')) if chars.next_if(|&(_, nc)| nc == '*').is_some() => depth += 1,
            Some(_) => {}
            None => return Err(()),
        }
    }

    let bytes_skipped = chars.peek().map_or(remainder.len(), |&(pos, _)| pos);
    lex.bump(bytes_skipped);

    Ok(())
}

#[derive(Logos, Debug, Clone, PartialEq, Eq, Copy)]
pub enum Token {
    // Delimiters
    #[token(";")]
    Semicolon,
    #[token(",")]
    Comma,
    #[token(":")]
    Colon,
    #[token(".")]
    Dot,
    #[token("{")]
    LeftCurly,
    #[token("}")]
    RightCurly,
    #[token("(")]
    LeftRound,
    #[token(")")]
    RightRound,
    #[token("[")]
    LeftSquare,
    #[token("]")]
    RightSquare,

    // Operators
    #[token("->")]
    ThinArrow,
    #[token("=")]
    Equals,
    #[token("+")]
    Plus,
    #[token("+%")]
    PlusPercent,
    #[token("-")]
    Minus,
    #[token("-%")]
    MinusPercent,
    #[token("*")]
    Star,
    #[token("*%")]
    StarPercent,
    #[token("/")]
    Slash,
    #[token("/+")]
    SlashPlus,
    #[token("/-")]
    SlashNeg,
    #[token("/<")]
    SlashLess,
    #[token("/>")]
    SlashGreater,
    #[token("%")]
    Percent,

    // Comparison
    #[token("==")]
    DoubleEquals,
    #[token("!=")]
    NotEquals,
    #[token("<")]
    LessThan,
    #[token(">")]
    GreaterThan,
    #[token("<=")]
    LessEquals,
    #[token(">=")]
    GreaterEquals,

    // Logical
    #[token("&&")]
    And,
    #[token("||")]
    Or,
    #[token("!")]
    Not,

    // Bitwise
    #[token("&")]
    Ampersand,
    #[token("|")]
    Pipe,
    #[token("^")]
    Caret,
    #[token("~")]
    Tilde,
    #[token("<<")]
    ShiftLeft,
    #[token(">>")]
    ShiftRight,

    // Keywords
    #[token("if")]
    If,
    #[token("else")]
    Else,
    #[token("fn")]
    Fn,
    #[token("let")]
    Let,
    #[token("mut")]
    Mut,
    #[token("const")]
    Const,
    #[token("init")]
    Init,
    #[token("run")]
    Run,
    #[token("struct")]
    Struct,
    #[token("return")]
    Return,
    #[token("comptime")]
    Comptime,
    #[token("inline")]
    Inline,
    #[token("while")]
    While,
    #[token("true")]
    True,
    #[token("false")]
    False,

    #[regex("[a-zA-Z_][a-zA-Z0-9_]*")]
    Identifier,

    #[regex("-?[0-9][0-9_]*")]
    DecLiteral,
    #[regex("-?0x[0-9A-Fa-f][0-9A-Fa-f_]*")]
    HexLiteral,
    #[regex("-?0b[01][01_]*")]
    BinLiteral,

    #[regex("[ \t\n\r]+")]
    Whitespace,
    #[token(r"//", lex_line_comment)]
    LineComment,
    #[token(r"/*", lex_block_comment)]
    BlockComment,

    Eof,
    Error,
}

pub type SourceSpan = Span<u32>;

#[derive(Debug, Clone)]
pub struct Lexer<'src>(LogosLexer<'src, Token>);

impl<'src> Lexer<'src> {
    pub fn new(source: &'src str) -> Self {
        _ = u32::try_from(source.len()).expect("source exceeds max len of 2^32-1");
        Self(Token::lexer(source))
    }
}

impl<'src> Iterator for Lexer<'src> {
    type Item = (Token, SourceSpan);

    fn next(&mut self) -> Option<Self::Item> {
        let tok = self.0.next()?.unwrap_or(Token::Error);
        let span = self.0.span();

        Some((tok, Span::new(span.start as u32, span.end as u32)))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    trait AsUsizeRange {
        fn range(self) -> std::ops::Range<usize>;
    }

    impl AsUsizeRange for SourceSpan {
        fn range(self) -> std::ops::Range<usize> {
            self.start as usize..self.end as usize
        }
    }

    fn lex_all(source: &str) -> Vec<(Token, SourceSpan)> {
        Lexer::new(source).collect()
    }

    fn lex_tokens(source: &str) -> Vec<Token> {
        Lexer::new(source).map(|(tok, _)| tok).collect()
    }

    #[test]
    fn test_delimiters() {
        let source = "; , : . { } ( ) [ ]";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::Semicolon,
                Token::Comma,
                Token::Colon,
                Token::Dot,
                Token::LeftCurly,
                Token::RightCurly,
                Token::LeftRound,
                Token::RightRound,
                Token::LeftSquare,
                Token::RightSquare,
            ]
        );
    }

    #[test]
    fn test_operators() {
        let source = "-> = + - * / %";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::ThinArrow,
                Token::Equals,
                Token::Plus,
                Token::Minus,
                Token::Star,
                Token::Slash,
                Token::Percent,
            ]
        );
    }

    #[test]
    fn test_comparisons() {
        let source = "== != < > <= >=";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::DoubleEquals,
                Token::NotEquals,
                Token::LessThan,
                Token::GreaterThan,
                Token::LessEquals,
                Token::GreaterEquals,
            ]
        );
    }

    #[test]
    fn test_logical() {
        let source = "&& || !";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::And, Token::Or, Token::Not,]);
    }

    #[test]
    fn test_bitwise() {
        let source = "& | ^ ~ << >>";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::Ampersand,
                Token::Pipe,
                Token::Caret,
                Token::Tilde,
                Token::ShiftLeft,
                Token::ShiftRight,
            ]
        );
    }

    #[test]
    fn test_keywords() {
        let source =
            "if else fn let mut const init run struct return comptime inline while true false";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::If,
                Token::Else,
                Token::Fn,
                Token::Let,
                Token::Mut,
                Token::Const,
                Token::Init,
                Token::Run,
                Token::Struct,
                Token::Return,
                Token::Comptime,
                Token::Inline,
                Token::While,
                Token::True,
                Token::False,
            ]
        );
    }

    #[test]
    fn test_identifiers() {
        let source = "foo Bar _underscore x1 var_2";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
            ]
        );
    }

    #[test]
    fn test_identifier_not_keyword() {
        let source = "ifx elsewhere fns letter mutable constant";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
                Token::Identifier,
            ]
        );
    }

    #[test]
    fn test_decimal_literals() {
        let source = "123 1_000 0 42";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![Token::DecLiteral, Token::DecLiteral, Token::DecLiteral, Token::DecLiteral,]
        );
    }

    #[test]
    fn test_negative_decimal_literals() {
        let source = "-42 -1_000";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::DecLiteral, Token::DecLiteral,]);
    }

    #[test]
    fn test_hex_literals() {
        let source = "0xFF 0x1A_BC 0xdead 0x0";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![Token::HexLiteral, Token::HexLiteral, Token::HexLiteral, Token::HexLiteral,]
        );
    }

    #[test]
    fn test_negative_hex_literals() {
        let source = "-0xDEAD -0xff";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::HexLiteral, Token::HexLiteral,]);
    }

    #[test]
    fn test_binary_literals() {
        let source = "0b101 0b1010_0011 0b0 0b1";
        let tokens = lex_tokens(source);
        assert_eq!(
            tokens,
            vec![Token::BinLiteral, Token::BinLiteral, Token::BinLiteral, Token::BinLiteral,]
        );
    }

    #[test]
    fn test_negative_binary_literals() {
        let source = "-0b11 -0b1010";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::BinLiteral, Token::BinLiteral,]);
    }

    #[test]
    fn test_line_comment_skipped() {
        let source = "// this is a comment\nfoo";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error, Token::Identifier]);
    }

    #[test]
    fn test_line_comment_at_eof() {
        let source = "foo // comment at end";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier]);
    }

    #[test]
    fn test_block_comment_skipped() {
        let source = "/* comment */ foo";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier]);
    }

    #[test]
    fn test_nested_block_comment() {
        let source = "/* outer /* inner */ outer */ bar";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier]);
    }

    #[test]
    fn test_block_comment_utf8() {
        let source = "/* Ã©moji ðŸŽ‰ */ foo";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier]);
    }

    #[test]
    fn test_block_comment_utf8_cjk() {
        let source = "/* æ—¥æœ¬èªž */ bar";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier]);
    }

    #[test]
    fn test_line_comment_utf8() {
        let source = "// cafÃ© â˜•\nfoo";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error, Token::Identifier]);
    }

    #[test]
    fn test_invalid_char_at() {
        let source = "@";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error]);
    }

    #[test]
    fn test_invalid_char_hash() {
        let source = "#";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error]);
    }

    #[test]
    fn test_invalid_char_in_context() {
        let source = "foo @ bar";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Identifier, Token::Error, Token::Identifier]);
    }

    #[test]
    fn test_unclosed_block_comment() {
        let source = "/* no end";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error, Token::Identifier, Token::Identifier]);
    }

    #[test]
    fn test_unclosed_nested_block_comment() {
        let source = "/* outer /* inner */";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::Error, Token::Identifier]);
    }

    #[test]
    fn test_malformed_binary_no_digits() {
        let source = "0b__";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::DecLiteral, Token::Identifier]);
    }

    #[test]
    fn test_malformed_binary_prefix_only() {
        let source = "0b";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::DecLiteral, Token::Identifier]);
    }

    #[test]
    fn test_malformed_hex_prefix_only() {
        let source = "0x";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::DecLiteral, Token::Identifier]);
    }

    #[test]
    fn test_malformed_hex_no_digits() {
        let source = "0x__";
        let tokens = lex_tokens(source);
        assert_eq!(tokens, vec![Token::DecLiteral, Token::Identifier]);
    }

    #[test]
    fn test_span_identifier() {
        let source = "  foo  bar  ";
        let results = lex_all(source);
        assert_eq!(results.len(), 2);
        assert_eq!(&source[results[0].1.range()], "foo");
        assert_eq!(&source[results[1].1.range()], "bar");
    }

    #[test]
    fn test_span_numeric_literals() {
        let source = "  123  0xFF  0b101  ";
        let results = lex_all(source);
        assert_eq!(results.len(), 3);
        assert_eq!(&source[results[0].1.range()], "123");
        assert_eq!(&source[results[1].1.range()], "0xFF");
        assert_eq!(&source[results[2].1.range()], "0b101");
    }

    #[test]
    fn test_span_multi_char_operators() {
        let source = "  ->  ==  <=  <<  ";
        let results = lex_all(source);
        assert_eq!(results.len(), 4);
        assert_eq!(&source[results[0].1.range()], "->");
        assert_eq!(&source[results[1].1.range()], "==");
        assert_eq!(&source[results[2].1.range()], "<=");
        assert_eq!(&source[results[3].1.range()], "<<");
    }
}
